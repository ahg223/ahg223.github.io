---
title: "Deep Learning"
excerpt: "Summary"

categories:
  - ModernCS
tags:
  - ModernCS
  - DeepLearning
---

## Sigmoid vs ReLU

sigmoid의 장점은 값을 0에서 1사이의 값으로 출력할 수 있어 확률로 표현할 때 유용하다는 점 이고 단점으로는 역전파시에 입력층에 가까운 웨이트들은 체인룰에 의해서 점점 값이 작아져 잘 학습이 되지 않는다는 단점이 있다. Relu는 그에 반해 체인룰을 적용하더라도 기울기 값이 0과 1로 만 이루어져 있어 그래디언트 소실문제가 해결되고, 미분 연산도 상당히 빠르다. Relu의 단점으로 Dying 노드가 생길 수 있는데 초기 학습속도가 너무 클 경우 출력값이 0이하로 될시에 계속해서 0 으로 나가게 된다.  

### Non-Linearity라는 말의 의미와 그 필요성은? 
Linearity란 선형성을 의미하는데 데이터 클래스를 나누는 문제에서 직선으로 클래스를 구분하는 것을 말한다. 딥러닝에서 결국 각 레이어는 y = wx + b 연산을 하는데, Non-linear한 활성함수를 쓰지 않는다면 여러 레이어를 행렬곱으로 나타내면 결 국 하나의 레이어로 압축될 수 있게된다. 따라서 Non-linearity란 각 레이어의 출력을 곡선함수로 근사해 각 레이어의 역할을 유니크하게 한다.  

### ReLU의 문제점은? 
Dying Relu문제. Relu의 입력값이 0보다 작게되면 입력값에 상관없이 0을 출력 하게 되는데, 초기에 W가 잘못 초기화되거나 러닝 레이트가 너무 커 절대값이 크게 학습될경우 어 떤 값과 곱해져도 계속해서 0보다 작은 값을 만들 수 있다. 이 때 Relu는 항상 0을 출력하고 기울기 도 0으로 학습도 되지 않기 때문에 Dying Relu라고 부른다.  

### Bias는 왜 있는걸까? 
활성함수를 좌우로 움직여주는 역할을 할 수 있다. 따라서 데이터에 따라서 편향값을 학습함으로써 원하는 어떤 역치값을 조정해줄 수 있다.    

## Gradient Descent 
신경망에서 어떤 파라미터가 결과에 얼마나 영향을 미쳤는지를 파악해 학습할 정도를 조정해주는 방식이다.  

### 왜 꼭 Gradient를 써야 할까? 
그래디언트를 사용하지 않는다고 가정한다면 학습은 파라미터를 랜 덤으로 조정하면서 loss값이 낮아지는 지점을 탐색하는 수준일 것이다. 그런데 탐색할 파라미터가 많아지면 많아질수록 이 방식은 무한한 연산속에 놓이게 된다. 따라서 그래디언트 방식이 효과적 이다.  

### 그래프에서 가로축과 세로축 각각은 무엇인가?
각 파라미터들.  

### 실제 상황에서는 그 그래프가 어떻게 그려질까? 
영향을 미치는 파라미터의 수가 훨씬 많기 때문에 다차원으로 그려질 것이다.    

### GD 중에 때때로 Loss가 증가하는 이유는? 
어떤 로컬 옵티마를 찾는 과정에서 러닝 레이트가 클 경 우 로컬 옵티마를 지나치게되면 그럴 수 있다.    

### 중학생이 이해할 수 있게 더 쉽게 설명 한다면? 
골프를 칠 때 홀을 겨냥해서 공을 치는데, 거리가 멀 면 더 힘을 많이 주기도 하고 거리가 가까우면 퍼팅을 하기도 한다. 그런데 이 때 힘 조절에 실패하면 구멍을 겨냥해 치더라도 공이 구멍을 지나쳐 더 멀어지기도 한다.  

### Back Propagation에 대해서 쉽게 설명 한다면? 
각 파라미터가 최종 결과에 대해 오차를 만드는 정 도를 계산해서 그 영향도 만큼씩 가중치를 주어 값을 조정하는 학습 방식이다.  

## CNN  
CNN은 MLP와 비교해봐야 하는데, MLP가 이미지의 지역적 특 색을 무시하고 한 줄의 벡터로 이미지를 표현함으로써 성능향상을 포기하는 부분이 있다. CNN은 커널을 사용하여 입력 이미지 혹은 피쳐를 도장처럼 훑으면서 각 지역적 특색을 보존하면서 학습 을 진행한다. 따라서 이미지 영역에서 MLP보다 훨씬 뛰어나다. 또한 연산 과정자체가 Non linear 하다.  

### 어떤 CNN의 파라메터 개수를 계산해 본다면? 
커널에 들어가는 웨이트와 bias 수.

### 주어진 CNN과 똑같은 MLP를 만들 수 있나? 
없음. 

### MLP는 Linear하고, CNN은 Non linear 풀링시에 만약 Max를 사용한다면 그 이유는?
해당 커널 영역내의 가장 중요한 특성을 뽑아낸다. 

### 시퀀스 데이터에 CNN을 적용하는 것이 가능할까? 
가능하다. CNN의 또 다른 특성으로 순서를 보 존한다는 특징이 있는데, 커널이 움직이는 방향에 일관된 순서가 반영되기 때문에 순서가 중요한 시퀀스 데이터에도 잘 적용될 수 있다. 실제로 자연어처리에서도 CNN을 사용해서 처리하는 방식 이 소개되었다.

## Word2Vec의 원리
어떤 단어가 내포하고있는 latent feacture가 있다고 가정합니다. 예를 들어 포도라는 단어는 보라색이라는 정보와 동그랗다는 정보, 과일이라는 정보 등등이 내포되어 있는 것이죠. 그래서 포도는 사과랑도 공통점이 있고, 보라색과도 공통점이 있죠. 이 latent feature들을 사람이 만들지 않고 임의로 생성해 비슷한 단어들끼리 가깝게 학습시켜준다면 자연어를 수치 벡터 로 표현할 수 있다는 원리입니다. 자연어를 수치 벡터로 표현하게 되면 덧셈, 곱셈, 거리 연산이 가능해지게 됩니다.

### 남자와 여자가 가까울까? 남자와 자동차가 가까울까? 
남자 여자가 더 가까울 겁니다. 남자와 여자 의 차이는 성별의 차이만 있다고 볼 수 있는데, 남자와 자동차와의 차이는 사람과 물체, 딱딱함 정도 등등이 다르기 때문입니다. 하지만 사실 이 문제는 데이터 분포와 학습방식에 따라 달라질 수 있기 때문에 학습하는 데이터 분포에서 남자와 여자가 더 가깝게 학습될지 남자와 자동차가 더 가깝 게 학습될지는 모릅니다.

### Unsupervised 러닝은? 
정답값을 주어지지 않고 학습시키는 방식으로 주로 차원 축소나 클러스터 링을 다룹니다. PCA의 경우 K-means 클러스터링의 경우 데이터를 K개의 클러스토로 분류하는 문 제인데요. 학습하는 방법은 임의의 K개의 점을 잡아 클러스터의 중심점으로 사용합니다. 각 데이 터는 가장 가까운 클러스터 중심점을 찾아 해당 클러스터로 분류되게 됩니다. 1차 분류가 끝난뒤 각 클러스터의 중앙에 가장 가까운 점을 새로운 중심점으로 잡아 위의 과정을 반복합니다.
### 딥러닝을 사용해서 PCA를 구현할수 있을까? 
Loss를 입력 피쳐와 출력 피쳐의 차이로 설정한다면 간단하게 가능하다.

## Auto Encoder
오토 인코더는 입력 피쳐에서 가장 중요한 피쳐들만 뽑아 적은 피쳐로 데이터를 압축하는 인코더와, 압축된 피쳐를 다시 원래 상태로 복원시키는 디코 더의 결합이다. 로스는 복원벡터와 입력벡터의 차의 MSE로 정의할 수 있다.

### 임베딩 차원을 늘렸을 때의 장단점은?
AE 학습시 항상 Loss를 0으로 만들수 있을까? 불가능할 것이다. 결국 피쳐를 더 적은 핵심 피쳐로 줄이는 것인데 디코더의 결과는 핵심 피쳐에 의해서 결정된다. 그런데 항상 서로 다른 입력이 서로 다른 핵심 피쳐로 매칭되는 것이 보장되는 것은 수학적으로는 가능할 수 있지만, 실제에서는 일어 나기 힘들다.

### VAE는 무엇인가? 
Variational Auto Encoder로 기존의 입력 피쳐를 핵심 피쳐로 줄인 후에, 핵심 피 쳐의 특성을 조금 바꾸었을 때 입력과 비슷한 새로운 출력이 나올 수 있게끔 학습시키는 모델이다. 일종의 Gan과 비슷하다. 이 때 핵심 피쳐의 평균과 표준편차를 사용하여 디코더의 입력을 만든다.

## Training 세트와 Test 세트 
결국 학습의 목적은 학습하지 않은 데이터에서도 잘 동작하는 제네럴한 성능을 가진 모델을 만드는 것이기 때문이다.

### Validation 세트가 따로 있는 이유는? 
학습시에 학습데이터에 오버피팅이 일어나고 있지 않은지 판 단하기 위해서.

###  Test 세트가 오염되었다는 말의 뜻은? 
모델이 테스트 데이터를 이미 학습해서 테스트 데이터의 객 관성이 떨어졌다는 의미

### Regularization이란 무엇인가? 
모델이 오버피팅이 되지 않고 범용성을 갖추도록 하는 기법이다. 일 반적으로 드롭 아웃, L2 Regularization등이 있다. L2 Regularization은 일반적으로 오버피팅이 특 정 파라미터의 값이 커졌을 때 잘 일어난다 보고 loss값에 파라미터의 제곱 크기를 더해주는 방법 이다.

## Batch Normalization 
BN은 각 레이어의 입력전에 입력 피쳐를 z 정규화를 하고 scale, shift를 시켜 입력 피쳐들의 분포를 안정적으로 만든다. 학습 속도가 빨라지고 오버피팅 확률이 줄 어든다.

### Dropout의 효과는? 
랜덤으로 선별된 적은 노드로도 성과를 내야하기 때문에 전체 모델이 오버피 팅 되는 경향이 줄어든다.

### BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 
학습시에는 mini-batch 단위의 평 균과 분산을 사용하게 되지만, 테스트시에는 미니 배치보다 적은 단위의 케이스를 다룰 수 있기 때 문에, 미리 학습 데이터에서 뽑아낸 평균과 분산을 사용해서 처리해야한다.

## SGD, RMSprop, Adam 
SGD는 미니 배치에서의 loss에 대해 기울 기를 계산해 학습하는 방식이고 RMSprop는 학습하면서 한 번에 움직이는 step size를 조정하는 방식인데 지금까지 많이 움직인 파라미터는 조금만 움직이도록 패널티를 주고, 적게 움직인 파라 미터가 더 많이 움직일 수 있도록 가중치를 주는 방식이다. Adam은 Adaptive momentum 방식으 로 step 사이즈를 조정하기도하고, 기존에 학습 방향에도 모멘텀을 주어서 전반적인 방향성에 가 중치를 주어 빠르게 미니마를 찾도록 한다.  

### SGD에서 Stochastic의 의미는? 
GD가 한 번 움직일때마다 모든 학습 데이터셋을 사용한다면 SGD 는 그 중 학습 데이터셋의 일부만 사용해서 움직입니다. Stocahstic은 어떤 샘플을 고를지가 확률 적으로 정해진다는 의미입니다.

### 미니배치를 작게 할때의 장단점은? 
gradient 계산 속도가 빨라지는 게 장점이고, 특정 데이터셋에 편향되어 움직일 수 있다는게 단점입니다.


## 딥러닝과 GPU 
GPU는 Matrix x Matrix, Matrix x vector 연산을 위한 최적의 기계이다. CUDA를 사용해서 매트릭스를 GPU에서 한 번에 연산할 수 있는 사이즈로 타일링을 시켜주면 GPU는 병렬적으로 1 사이클에 메모리에 올릴 수 있는 최대 연산을 할 수 있게된다. 또한 딥러닝 네트워크에는 웨이트가 0인 값이 상당히 많아지게 되는데, DMA, 제로 스키핑 기술과 같은 기술을 사용해서 연산 속도를 빠르게 할 수 있다. 따라서 CPU로 연산하는 것보다 훨씬 빠르다.

### 학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는? 
미니 배치 단위가 너무 작기 때문이다. 

### GPU를 두개 다 쓰고 싶다. 방법은? 
케라스에서는 그냥 멀티 지피유 코드를 사용하면 되지만, 텐서 플로에서는 네트웍을 나누어 각 GPU에 올려야한다. 만약 텐서플로에서 한 네트워크를 여러개의 GPU를 사용해서 처리속도를 높이고 싶다면, 각각의 GPU에 같은 모델을 올린 후에 서로 다른 미니 배치를 처리하고, 값을 평균내 학습을 시킨다던가 하는 기술이 필요하다.

### 학습시 필요한 GPU 메모리는 어떻게 계산하는가? 
첫째로 네트워크 사이즈를 체크해야 한다. 케라스 같은 경우에는 모델 서머리를 사용해서 전체 네트워크를 체크할 수 있고, 텐서플로도 비슷한 방법으로 네트워크 사이즈를 체크할 수 있다. 주의해야 할 점은, AdamOptimizer를 사용할 경우 필요한 네트웤 사이즈가 두 배가 된다. 둘 째로 미니 배치 단위의 메모리를 체크해야한다. 이 경우 한 번에 GPU 메모리에 올라가는 미니 배치의 입력 크기를 계산해 보면 된다.

## 디버깅 노하우 

TF를 사용할 때 tf.summary.scalar를 사용해 loss 나 validation accuracy값을 저장할 수 있고, tf.train.summarywriter를 사용해 텐서보드에서 값들 을 디버깅할 수 있다. 그리고 텐서보드에서 graph를 살펴보면서, 레이어들의 연결관계와 optimizer와 제대로 연결되어 있는지를 살펴볼 수 있다.
